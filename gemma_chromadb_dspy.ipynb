{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6266221,"sourceType":"datasetVersion","datasetId":3601853},{"sourceId":7771108,"sourceType":"datasetVersion","datasetId":9},{"sourceId":7529686,"sourceType":"datasetVersion","datasetId":4385612}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-06T14:02:30.394750Z","iopub.execute_input":"2024-03-06T14:02:30.395100Z","iopub.status.idle":"2024-03-06T14:02:30.772834Z","shell.execute_reply.started":"2024-03-06T14:02:30.395066Z","shell.execute_reply":"2024-03-06T14:02:30.771871Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/aurelien-geron-hands-on-machine-learning/Aurelien-Geron-Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-Tensorflow_-Concepts-Tools-and-Techniques-to-Build-Intelligent-Systems-OReilly-Media-2019.pdf\n/kaggle/input/meta-kaggle/KernelTags.csv\n/kaggle/input/meta-kaggle/KernelVersionCompetitionSources.csv\n/kaggle/input/meta-kaggle/Datasets.csv\n/kaggle/input/meta-kaggle/KernelVersionKernelSources.csv\n/kaggle/input/meta-kaggle/KernelVotes.csv\n/kaggle/input/meta-kaggle/Submissions.csv\n/kaggle/input/meta-kaggle/KernelLanguages.csv\n/kaggle/input/meta-kaggle/Users.csv\n/kaggle/input/meta-kaggle/ForumMessageVotes.csv\n/kaggle/input/meta-kaggle/Competitions.csv\n/kaggle/input/meta-kaggle/DatasetTaskSubmissions.csv\n/kaggle/input/meta-kaggle/UserAchievements.csv\n/kaggle/input/meta-kaggle/UserOrganizations.csv\n/kaggle/input/meta-kaggle/Teams.csv\n/kaggle/input/meta-kaggle/UserFollowers.csv\n/kaggle/input/meta-kaggle/CompetitionTags.csv\n/kaggle/input/meta-kaggle/Kernels.csv\n/kaggle/input/meta-kaggle/Organizations.csv\n/kaggle/input/meta-kaggle/Datasources.csv\n/kaggle/input/meta-kaggle/ForumTopics.csv\n/kaggle/input/meta-kaggle/DatasetVersions.csv\n/kaggle/input/meta-kaggle/DatasetVotes.csv\n/kaggle/input/meta-kaggle/TeamMemberships.csv\n/kaggle/input/meta-kaggle/Forums.csv\n/kaggle/input/meta-kaggle/KernelVersions.csv\n/kaggle/input/meta-kaggle/ForumMessages.csv\n/kaggle/input/meta-kaggle/KernelVersionDatasetSources.csv\n/kaggle/input/meta-kaggle/Episodes.csv\n/kaggle/input/meta-kaggle/EpisodeAgents.csv\n/kaggle/input/meta-kaggle/Tags.csv\n/kaggle/input/meta-kaggle/DatasetTasks.csv\n/kaggle/input/meta-kaggle/DatasetTags.csv\n/kaggle/input/llama2-7b-hf/Llama2-7b-hf/config.json\n/kaggle/input/llama2-7b-hf/Llama2-7b-hf/pytorch_model-00002-of-00002.bin\n/kaggle/input/llama2-7b-hf/Llama2-7b-hf/tokenizer.json\n/kaggle/input/llama2-7b-hf/Llama2-7b-hf/tokenizer_config.json\n/kaggle/input/llama2-7b-hf/Llama2-7b-hf/pytorch_model.bin.index.json\n/kaggle/input/llama2-7b-hf/Llama2-7b-hf/pytorch_model-00001-of-00002.bin\n/kaggle/input/llama2-7b-hf/Llama2-7b-hf/special_tokens_map.json\n/kaggle/input/llama2-7b-hf/Llama2-7b-hf/tokenizer.model\n/kaggle/input/llama2-7b-hf/Llama2-7b-hf/generation_config.json\n/kaggle/input/aurelien-geron-hands-on-machine-learning/Aurelien-Geron-Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-Tensorflow_-Concepts-Tools-and-Techniques-to-Build-Intelligent-Systems-OReilly-Media-2019.pdf\n/kaggle/input/meta-kaggle/KernelTags.csv\n/kaggle/input/meta-kaggle/KernelVersionCompetitionSources.csv\n/kaggle/input/meta-kaggle/Datasets.csv\n/kaggle/input/meta-kaggle/KernelVersionKernelSources.csv\n/kaggle/input/meta-kaggle/KernelVotes.csv\n/kaggle/input/meta-kaggle/Submissions.csv\n/kaggle/input/meta-kaggle/KernelLanguages.csv\n/kaggle/input/meta-kaggle/Users.csv\n/kaggle/input/meta-kaggle/ForumMessageVotes.csv\n/kaggle/input/meta-kaggle/Competitions.csv\n/kaggle/input/meta-kaggle/DatasetTaskSubmissions.csv\n/kaggle/input/meta-kaggle/UserAchievements.csv\n/kaggle/input/meta-kaggle/UserOrganizations.csv\n/kaggle/input/meta-kaggle/Teams.csv\n/kaggle/input/meta-kaggle/UserFollowers.csv\n/kaggle/input/meta-kaggle/CompetitionTags.csv\n/kaggle/input/meta-kaggle/Kernels.csv\n/kaggle/input/meta-kaggle/Organizations.csv\n/kaggle/input/meta-kaggle/Datasources.csv\n/kaggle/input/meta-kaggle/ForumTopics.csv\n/kaggle/input/meta-kaggle/DatasetVersions.csv\n/kaggle/input/meta-kaggle/DatasetVotes.csv\n/kaggle/input/meta-kaggle/TeamMemberships.csv\n/kaggle/input/meta-kaggle/Forums.csv\n/kaggle/input/meta-kaggle/KernelVersions.csv\n/kaggle/input/meta-kaggle/ForumMessages.csv\n/kaggle/input/meta-kaggle/KernelVersionDatasetSources.csv\n/kaggle/input/meta-kaggle/Episodes.csv\n/kaggle/input/meta-kaggle/EpisodeAgents.csv\n/kaggle/input/meta-kaggle/Tags.csv\n/kaggle/input/meta-kaggle/DatasetTasks.csv\n/kaggle/input/meta-kaggle/DatasetTags.csv\n/kaggle/input/llama2-7b-hf/Llama2-7b-hf/config.json\n/kaggle/input/llama2-7b-hf/Llama2-7b-hf/pytorch_model-00002-of-00002.bin\n/kaggle/input/llama2-7b-hf/Llama2-7b-hf/tokenizer.json\n/kaggle/input/llama2-7b-hf/Llama2-7b-hf/tokenizer_config.json\n/kaggle/input/llama2-7b-hf/Llama2-7b-hf/pytorch_model.bin.index.json\n/kaggle/input/llama2-7b-hf/Llama2-7b-hf/pytorch_model-00001-of-00002.bin\n/kaggle/input/llama2-7b-hf/Llama2-7b-hf/special_tokens_map.json\n/kaggle/input/llama2-7b-hf/Llama2-7b-hf/tokenizer.model\n/kaggle/input/llama2-7b-hf/Llama2-7b-hf/generation_config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install langchain chromadb pypdf sentence-transformers bitsandbytes accelerate","metadata":{"execution":{"iopub.status.busy":"2024-03-06T14:02:30.775494Z","iopub.execute_input":"2024-03-06T14:02:30.775953Z","iopub.status.idle":"2024-03-06T14:06:28.731018Z","shell.execute_reply.started":"2024-03-06T14:02:30.775920Z","shell.execute_reply":"2024-03-06T14:06:28.729714Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting langchain\n  Downloading langchain-0.1.11-py3-none-any.whl.metadata (13 kB)\nCollecting chromadb\n  Downloading chromadb-0.4.24-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: pypdf in /opt/conda/lib/python3.10/site-packages (4.0.2)\nCollecting sentence-transformers\n  Downloading sentence_transformers-2.5.1-py3-none-any.whl.metadata (11 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.27.2)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.33)\nCollecting langchain-community<0.1,>=0.0.25 (from langchain)\n  Downloading langchain_community-0.0.25-py3-none-any.whl.metadata (8.1 kB)\nCollecting langchain-core<0.2,>=0.1.29 (from langchain)\n  Downloading langchain_core-0.1.29-py3-none-any.whl.metadata (6.0 kB)\nCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.22-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nCollecting build>=1.0.3 (from chromadb)\n  Downloading build-1.1.1-py3-none-any.whl.metadata (4.2 kB)\nCollecting chroma-hnswlib==0.7.3 (from chromadb)\n  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\nRequirement already satisfied: fastapi>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.108.0)\nRequirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.25.0)\nCollecting posthog>=2.4.0 (from chromadb)\n  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.9.0)\nCollecting pulsar-client>=3.1.0 (from chromadb)\n  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\nCollecting onnxruntime>=1.14.1 (from chromadb)\n  Downloading onnxruntime-1.17.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.44b0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nRequirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.15.2)\nCollecting pypika>=0.48.9 (from chromadb)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.66.1)\nRequirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (7.4.0)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb) (6.1.1)\nCollecting grpcio>=1.58.0 (from chromadb)\n  Downloading grpcio-1.62.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting bcrypt>=4.0.1 (from chromadb)\n  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\nRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.9.0)\nCollecting kubernetes>=28.1.0 (from chromadb)\n  Downloading kubernetes-29.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: mmh3>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.1.0)\nCollecting orjson>=3.9.12 (from chromadb)\n  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.38.1)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.20.3)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nCollecting pyproject_hooks (from build>=1.0.3->chromadb)\n  Downloading pyproject_hooks-1.0.0-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.0.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.32.0.post1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.2.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\nRequirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\nRequirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.26.1)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\nRequirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\nRequirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\nRequirement already satisfied: urllib3>=1.24.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.26.18)\nRequirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.29->langchain) (4.2.0)\nCollecting packaging>=20.0 (from accelerate)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\nRequirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\nRequirement already satisfied: importlib-metadata<7.0,>=6.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (6.11.0)\nRequirement already satisfied: backoff<3.0.0,>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\nRequirement already satisfied: opentelemetry-proto==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\nCollecting opentelemetry-instrumentation-asgi==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.44b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.44b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.44b0-py3-none-any.whl.metadata (2.2 kB)\nCollecting opentelemetry-util-http==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.44b0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: setuptools>=16.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (69.0.3)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\nCollecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading asgiref-3.7.2-py3-none-any.whl.metadata (9.2 kB)\nINFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-instrumentation-asgi==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\nCollecting opentelemetry-util-http==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.43b0-py3-none-any.whl.metadata (2.5 kB)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.3.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.2.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\nDownloading langchain-0.1.11-py3-none-any.whl (807 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.5/807.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading chromadb-0.4.24-py3-none-any.whl (525 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m414.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading sentence_transformers-2.5.1-py3-none-any.whl (156 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m574.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0mm\n\u001b[?25hDownloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading build-1.1.1-py3-none-any.whl (19 kB)\nDownloading grpcio-1.62.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_community-0.0.25-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.1.29-py3-none-any.whl (252 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.6/252.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\nDownloading langsmith-0.1.22-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.17.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl (11 kB)\nDownloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl (14 kB)\nDownloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl (28 kB)\nDownloading opentelemetry_util_http-0.43b0-py3-none-any.whl (6.9 kB)\nDownloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB)\nDownloading asgiref-3.7.2-py3-none-any.whl (24 kB)\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pypika\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=49ed80ac5fdcb3b417b8e6a5eae847b2a530f082b516cec426bff1acb962744a\n  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\nSuccessfully built pypika\nInstalling collected packages: pypika, monotonic, pyproject_hooks, pulsar-client, packaging, orjson, opentelemetry-util-http, humanfriendly, grpcio, chroma-hnswlib, bcrypt, asgiref, posthog, coloredlogs, build, bitsandbytes, opentelemetry-instrumentation, onnxruntime, langsmith, kubernetes, opentelemetry-instrumentation-asgi, langchain-core, sentence-transformers, opentelemetry-instrumentation-fastapi, langchain-text-splitters, langchain-community, langchain, chromadb\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n  Attempting uninstall: grpcio\n    Found existing installation: grpcio 1.51.1\n    Uninstalling grpcio-1.51.1:\n      Successfully uninstalled grpcio-1.51.1\n  Attempting uninstall: kubernetes\n    Found existing installation: kubernetes 26.1.0\n    Uninstalling kubernetes-26.1.0:\n      Successfully uninstalled kubernetes-26.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.8.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.1.2 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 29.0.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed asgiref-3.7.2 bcrypt-4.1.2 bitsandbytes-0.42.0 build-1.1.1 chroma-hnswlib-0.7.3 chromadb-0.4.24 coloredlogs-15.0.1 grpcio-1.60.0 humanfriendly-10.0 kubernetes-29.0.0 langchain-0.1.11 langchain-community-0.0.25 langchain-core-0.1.29 langchain-text-splitters-0.0.1 langsmith-0.1.22 monotonic-1.6 onnxruntime-1.17.1 opentelemetry-instrumentation-0.43b0 opentelemetry-instrumentation-asgi-0.43b0 opentelemetry-instrumentation-fastapi-0.43b0 opentelemetry-util-http-0.43b0 orjson-3.9.15 packaging-23.2 posthog-3.5.0 pulsar-client-3.4.0 pypika-0.48.9 pyproject_hooks-1.0.0 sentence-transformers-2.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install dspy-ai","metadata":{"execution":{"iopub.status.busy":"2024-03-06T14:06:49.216075Z","iopub.execute_input":"2024-03-06T14:06:49.216370Z","iopub.status.idle":"2024-03-06T14:07:02.544118Z","shell.execute_reply.started":"2024-03-06T14:06:49.216344Z","shell.execute_reply":"2024-03-06T14:07:02.543052Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: dspy-ai in /opt/conda/lib/python3.10/site-packages (2.3.6)\nRequirement already satisfied: backoff~=2.2.1 in /opt/conda/lib/python3.10/site-packages (from dspy-ai) (2.2.1)\nRequirement already satisfied: joblib~=1.3.2 in /opt/conda/lib/python3.10/site-packages (from dspy-ai) (1.3.2)\nRequirement already satisfied: openai<2.0.0,>=0.28.1 in /opt/conda/lib/python3.10/site-packages (from dspy-ai) (1.13.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from dspy-ai) (2.1.4)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from dspy-ai) (2023.12.25)\nRequirement already satisfied: ujson in /opt/conda/lib/python3.10/site-packages (from dspy-ai) (5.9.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from dspy-ai) (4.66.1)\nRequirement already satisfied: datasets<3.0.0,~=2.14.6 in /opt/conda/lib/python3.10/site-packages (from dspy-ai) (2.14.7)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from dspy-ai) (2.31.0)\nRequirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (from dspy-ai) (3.5.0)\nRequirement already satisfied: pydantic==2.5.0 in /opt/conda/lib/python3.10/site-packages (from dspy-ai) (2.5.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic==2.5.0->dspy-ai) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.1 in /opt/conda/lib/python3.10/site-packages (from pydantic==2.5.0->dspy-ai) (2.14.1)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic==2.5.0->dspy-ai) (4.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (1.26.4)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (11.0.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (0.6)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (0.3.7)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (0.70.15)\nRequirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets<3.0.0,~=2.14.6->dspy-ai) (2023.10.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (3.9.1)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (0.20.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (6.0.1)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (4.2.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (0.27.0)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=0.28.1->dspy-ai) (1.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->dspy-ai) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->dspy-ai) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->dspy-ai) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->dspy-ai) (2024.2.2)\nRequirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna->dspy-ai) (1.13.1)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna->dspy-ai) (6.8.2)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna->dspy-ai) (2.0.25)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->dspy-ai) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->dspy-ai) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->dspy-ai) (2023.4)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna->dspy-ai) (1.3.2)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=0.28.1->dspy-ai) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai) (4.0.3)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=0.28.1->dspy-ai) (1.0.4)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=0.28.1->dspy-ai) (0.14.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets<3.0.0,~=2.14.6->dspy-ai) (3.13.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->dspy-ai) (1.16.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna->dspy-ai) (3.0.3)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna->dspy-ai) (2.1.3)\n","output_type":"stream"}]},{"cell_type":"code","source":"import chromadb\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import Chroma\nfrom chromadb.utils import embedding_functions\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain.prompts import PromptTemplate\nfrom pypdf import PdfReader\nimport torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.vectorstores import Chroma\nfrom langchain_community.llms import LlamaCpp\nfrom langchain.document_loaders import TextLoader\nfrom langchain.llms import CTransformers\nfrom torch import cuda, bfloat16\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\nfrom chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\nfrom chromadb.config import Settings","metadata":{"execution":{"iopub.status.busy":"2024-03-06T14:07:02.546573Z","iopub.execute_input":"2024-03-06T14:07:02.546905Z","iopub.status.idle":"2024-03-06T14:07:27.356526Z","shell.execute_reply.started":"2024-03-06T14:07:02.546877Z","shell.execute_reply":"2024-03-06T14:07:27.355775Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-03-06 14:07:13.588937: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-06 14:07:13.589078: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-06 14:07:13.755076: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"def load_doc(path):\n    reader = PdfReader(path)\n    #We will iterate over each page 'p', extract the text and removing the whitespaces before and after sentences.\n    pdf_texts = [p.extract_text().strip() for p in reader.pages]\n\n    # Filter the empty strings-->pdf_texts will have only those pages that have the text\n    pdf_texts = [text for text in pdf_texts if text]\n\n    return pdf_texts","metadata":{"execution":{"iopub.status.busy":"2024-03-06T14:10:11.383321Z","iopub.execute_input":"2024-03-06T14:10:11.383705Z","iopub.status.idle":"2024-03-06T14:10:11.389735Z","shell.execute_reply.started":"2024-03-06T14:10:11.383670Z","shell.execute_reply":"2024-03-06T14:10:11.388677Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def chunks(path_of_doc):\n\n    #we will use the path of doc to pass it to load_doc\n    pdf_texts= load_doc(path_of_doc)\n    character_splitter = RecursiveCharacterTextSplitter(\n    #RecursiveCharacterTextSplitter will split the Document into chunks firstly when it will find the double line\n        #After that if the splitted chunks are greater than size of 1000 which is our chunk size they will get split on single line\n            #Even if Chunks have larger size than 1000 they will get split on \". \"\n        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n        #every chunk will have 2000 characters\n        chunk_size=2000,\n        chunk_overlap=20\n    )\n    #further splitting according to embedding model we are using\n    character_split_texts = character_splitter.split_text('\\n\\n'.join(pdf_texts))\n    #so lets split it according to our embedding model\n                                                                            #we want every token to have 256 characters\n    token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=256)\n    token_split_texts = []\n    for text in character_split_texts:\n    #splitting the text and storing in list\n        token_split_texts += token_splitter.split_text(text)\n\n    return token_split_texts","metadata":{"execution":{"iopub.status.busy":"2024-03-06T14:10:11.390942Z","iopub.execute_input":"2024-03-06T14:10:11.391231Z","iopub.status.idle":"2024-03-06T14:10:11.407419Z","shell.execute_reply.started":"2024-03-06T14:10:11.391207Z","shell.execute_reply":"2024-03-06T14:10:11.406682Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def embedding_function():\n    embedding_function = SentenceTransformerEmbeddingFunction()\n    return embedding_function","metadata":{"execution":{"iopub.status.busy":"2024-03-06T14:10:11.408613Z","iopub.execute_input":"2024-03-06T14:10:11.408972Z","iopub.status.idle":"2024-03-06T14:10:11.419759Z","shell.execute_reply.started":"2024-03-06T14:10:11.408942Z","shell.execute_reply":"2024-03-06T14:10:11.418861Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def doc_ingestion_in_vecdb(path_of_doc):\n    #we will pass the path of doc to chunks that will further pass it to doc_load for loading it\n    token_split_texts= chunks(path_of_doc)\n    embedding_func= embedding_function()\n\n    #making chromadb client object good for testing only not for production purpose\n#     chroma_client = chromadb.Client()\n    chroma_client = chromadb.PersistentClient(path=\"./kaggle/working/chromaDB\")\n\n    #making the collection of chroma database\n    chroma_collection = chroma_client.create_collection(\"Doc-Collection\", embedding_function=embedding_func)\n\n    #ids of each chunk\n    ids = [str(i) for i in range(len(token_split_texts))]\n\n    chroma_collection.add(ids=ids, documents=token_split_texts)\n    return chroma_collection","metadata":{"execution":{"iopub.status.busy":"2024-03-06T14:10:11.423008Z","iopub.execute_input":"2024-03-06T14:10:11.423264Z","iopub.status.idle":"2024-03-06T14:10:11.431386Z","shell.execute_reply.started":"2024-03-06T14:10:11.423242Z","shell.execute_reply":"2024-03-06T14:10:11.430623Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def retrieve_doc(query, chroma_collection):\n    #we will pass the document path to doc_ingestion_in_vecdb() so that can ingest it in vector db after chunking and loading\n    # chroma_collection= doc_ingestion_in_vecdb(path_to_doc)\n\n    #lets get the 5 relevant results\n    results = chroma_collection.query(query_texts=[query], n_results=5)\n\n    #[0] means give the result of the first query, right now we have only 1 query\n    retrieved_documents = results['documents'][0]\n\n    return retrieved_documents","metadata":{"execution":{"iopub.status.busy":"2024-03-06T14:10:11.432370Z","iopub.execute_input":"2024-03-06T14:10:11.432616Z","iopub.status.idle":"2024-03-06T14:10:11.447537Z","shell.execute_reply.started":"2024-03-06T14:10:11.432594Z","shell.execute_reply":"2024-03-06T14:10:11.446674Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def rag(query, retrieved_documents, model, tokenizer):\n\n    #we will join the retrieved documents (that are the relavant documents) into one variable\n    messages = f\"\"\"\n          You are a helpful expert of Machine Learning and Deep Learning.\n          Your users are asking questions about information contained in an Machine Learning and Deep Learning Book.\"\n          You will be shown the user's question, and the relevant information from the book.\n          Answer the user's question using only this relevant information. Try to build a good answer uing this information.\n\n          Give Precise answer according to User's Instruction, like if user wants explaination then provide explaination.\n          If User want an answer in one or two lines then give answer accordingly.\n\n        User's Question: {query}. \\n Information from the book of Machine Learning and Deep Learning: {retrieved_documents}\"\n    \"\"\"\n    pipe = pipeline(\"text-generation\",\n                        model=model,\n                        tokenizer=tokenizer)\n\n    llm = HuggingFacePipeline(pipeline=pipe)\n    # checking again that our model is working fine--->Asking LLM model the same question we asked our document\n    content= llm(prompt=messages)\n\n    return content","metadata":{"execution":{"iopub.status.busy":"2024-03-06T14:10:11.448509Z","iopub.execute_input":"2024-03-06T14:10:11.448766Z","iopub.status.idle":"2024-03-06T14:10:11.459467Z","shell.execute_reply.started":"2024-03-06T14:10:11.448743Z","shell.execute_reply":"2024-03-06T14:10:11.458562Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"path_to_doc =\"/kaggle/input/aurelien-geron-hands-on-machine-learning/Aurelien-Geron-Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-Tensorflow_-Concepts-Tools-and-Techniques-to-Build-Intelligent-Systems-OReilly-Media-2019.pdf\"\nchroma_collection = doc_ingestion_in_vecdb(path_to_doc)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T14:10:11.460559Z","iopub.execute_input":"2024-03-06T14:10:11.460868Z","iopub.status.idle":"2024-03-06T14:11:51.634115Z","shell.execute_reply.started":"2024-03-06T14:10:11.460844Z","shell.execute_reply":"2024-03-06T14:11:51.633319Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02b51984b7ec4d37bedef0bb33f8483a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6a2c72c13c241c1bb126d28c590cfc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d97b2dc3cb5e409eb41e082729329df7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fdc5bf0a8e44611a9a043d479ad113b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccad0b3ccff74879a5d46dc56f3f920b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"076408b69242424caf8480031562294b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56ebc927372e4cdba3aea193a9871c93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85b4a392580c477b9f2229440fbfc96b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93dd117d25fb4c1b8a7af3d7b4ffcc5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90f25152b4414c528ff4372dcd5c58d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22da94d2ef4641c68497696fb6411b92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f68a21ecb69b4c9ba6dcc9feff18042a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c940c9e0d25469fa2b90cf13c83424b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bfb7f8b16b94a37a936ac7a6e5d0483"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d53cf4415934ba085dc3a0679a22f2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41718bc68c7a45b68f4ebbfc613d77d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08d60d5744ac42b9b6cb9ecbaccbebcc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1a52566cf124bfb9f017c99a46c2941"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e21e9e96a2545b3965b486f0c7ddf45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0aa2f65ba8b9484eac7a4631602d01e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d54d94b4f33446f58c8a72a73f9fe657"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"137cbfdddde245b1adfc6b7550818134"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/40 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39d99d4d422c4e4f8900e6fdc28ca2bf"}},"metadata":{}}]},{"cell_type":"code","source":"import dspy","metadata":{"execution":{"iopub.status.busy":"2024-03-06T14:11:51.635369Z","iopub.execute_input":"2024-03-06T14:11:51.635660Z","iopub.status.idle":"2024-03-06T14:11:53.059933Z","shell.execute_reply.started":"2024-03-06T14:11:51.635624Z","shell.execute_reply":"2024-03-06T14:11:53.058952Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_API_KEY\")\n\n# Login to Hugging Face\nlogin(hf_token)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T14:11:53.061392Z","iopub.execute_input":"2024-03-06T14:11:53.061686Z","iopub.status.idle":"2024-03-06T14:11:53.301621Z","shell.execute_reply.started":"2024-03-06T14:11:53.061653Z","shell.execute_reply":"2024-03-06T14:11:53.300606Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"llm=dspy.HFModel(\"google/gemma-2b-it\")","metadata":{"execution":{"iopub.status.busy":"2024-03-06T14:11:53.302698Z","iopub.execute_input":"2024-03-06T14:11:53.302980Z","iopub.status.idle":"2024-03-06T14:12:24.634224Z","shell.execute_reply.started":"2024-03-06T14:11:53.302955Z","shell.execute_reply":"2024-03-06T14:12:24.633505Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aae7c725e1c1421d9d8d3b8268eaea81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ed5356474274839826184934f35e012"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"073d619637fd47ee9b2e907dbf61f841"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76bd7b601ff848ca9cdcfd245a9cf77e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/888 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f825a9c959394776882ad7cf289318e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f1dbf1b1c1143e4af9cff36d2de3079"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"855556c1fc8f4496bd68f68280d7e085"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e6f17eca6ec49b7a08a3e8bfc475268"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65324e87bf6a4b408a9b9e1a18f588f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b11b05ca36f4a208146cdc2bf1377b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a640dfb163b4cb9b3b5859350dd9fa0"}},"metadata":{}}]},{"cell_type":"code","source":"from dspy.retrieve.chromadb_rm import ChromadbRM","metadata":{"execution":{"iopub.status.busy":"2024-03-06T14:12:24.635305Z","iopub.execute_input":"2024-03-06T14:12:24.635649Z","iopub.status.idle":"2024-03-06T14:12:24.640465Z","shell.execute_reply.started":"2024-03-06T14:12:24.635605Z","shell.execute_reply":"2024-03-06T14:12:24.639560Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"rm = ChromadbRM(collection_name=\"Doc-Collection\",embedding_function=embedding_function(),persist_directory=\"./kaggle/working/chromaDB\")","metadata":{"execution":{"iopub.status.busy":"2024-03-06T14:12:24.641521Z","iopub.execute_input":"2024-03-06T14:12:24.641788Z","iopub.status.idle":"2024-03-06T14:12:25.452517Z","shell.execute_reply.started":"2024-03-06T14:12:24.641765Z","shell.execute_reply":"2024-03-06T14:12:25.451397Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"dspy.settings.configure(lm=llm,\n                       rm=rm)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T14:12:25.453731Z","iopub.execute_input":"2024-03-06T14:12:25.456993Z","iopub.status.idle":"2024-03-06T14:12:25.523076Z","shell.execute_reply.started":"2024-03-06T14:12:25.456966Z","shell.execute_reply":"2024-03-06T14:12:25.521892Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class RAG(dspy.Module):\n    def __init__(self,num_passages=5):\n        super().__init__()\n        self.retrive = dspy.Retrieve(k =num_passages)\n        self.generate_answer = dspy.ChainOfThought(\"context,question -> answer\")\n    def forward(self,question):\n        context = self.retrive(question).passages\n        prediction = self.generate_answer(context=context, question=question)\n        return dspy.Prediction(context=context, answer=prediction.answer)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T14:12:25.525849Z","iopub.execute_input":"2024-03-06T14:12:25.526240Z","iopub.status.idle":"2024-03-06T14:12:25.636588Z","shell.execute_reply.started":"2024-03-06T14:12:25.526211Z","shell.execute_reply":"2024-03-06T14:12:25.635580Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# without reranking\nexample_query=\"What is Scikit-Learn ?\"\nuncompiled_rag = RAG()\n\nresponse = uncompiled_rag(example_query)\n\nprint(response.answer)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T14:12:25.637902Z","iopub.execute_input":"2024-03-06T14:12:25.638220Z","iopub.status.idle":"2024-03-06T14:12:54.170495Z","shell.execute_reply.started":"2024-03-06T14:12:25.638182Z","shell.execute_reply":"2024-03-06T14:12:54.169562Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cbcc41e9b9640e1ac6600467bce6e3a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"${answer}\n\n---\n\nContext:\n[1] «1by default scikit - learn caches downloaded datasets in a directory called $ home / scikit _ learn _ data. chapter 3 classification with early release ebooks, you get books in their earliest form — the author ’ s raw and unedited content as he or she writes — so you can take advantage of these technologies long before the official release of these titles. the following will be chapter 3 in the final release of the book. in chapter 1 we mentioned that the most common supervised learning tasks are regression ( predicting values ) and classification ( predicting classes ). in chapter 2 we explored a regression task, predicting housing values, using various algorithms such as linear regression, decision trees, and random forests ( which will be explained in further detail in later chapters ). now we will turn our attention to classification systems. mnist in this chapter, we will be using the mnist dataset, which is a set of 70, 000 small images of digits handwritten by high school students and employees of the us cen ‐ sus bureau. each image is labeled with the digit it represents. this set has been stud ‐ ied so much that it is often called the “ hello world ” of machine learning : whenever people come up with a new classification algorithm,»\n[2] «they are curious to see how it will perform on mnist. whenever someone learns machine learning, sooner or later they tackle mnist. scikit - learn provides many helper functions to download popular datasets. mnist is one of them. the following code fetches the mnist dataset : 1 87»\n[3] «, or else it uses the full svd approach. if you want to force scikit - learn to use full svd, you can set the svd _ solver hyperparameter to \" full \". incremental pca one problem with the preceding implementations of pca is that they require the whole training set to fit in memory in order for the algorithm to run. fortunately, incremental pca ( ipca ) algorithms have been developed : you can split the training set into mini - batches and feed an ipca algorithm one mini - batch at a time. this is pca | 227»\n[4] «18some predictors also provide methods to measure the confidence of their predictions. 19this class is available since scikit - learn 0. 20. if you use an earlier version, please consider upgrading, or use pandas ’ series. factorize ( ) method. a test set ( and the corresponding labels in the case of supervised learning algorithms ). 18 • inspection. all the estimator ’ s hyperparameters are accessible directly via public instance variables ( e. g., imputer. strategy ), and all the estimator ’ s learned parameters are also accessible via public instance variables with an underscore suffix ( e. g., imputer. statistics _ ). • nonproliferation of classes. datasets are represented as numpy arrays or scipy sparse matrices, instead of homemade classes. hyperparameters are just regular python strings or numbers. • composition. existing building blocks are reused as much as possible. for example, it is easy to create a pipeline estimator from an arbitrary sequence of transformers followed by a final estimator, as we will see. • sensible defaults. scikit - learn provides reasonable default values for most parameters, making it easy to create a baseline working system quickly. handling text»\n[5] «scikit - learn detects when you try to use a binary classification algorithm for a multi ‐ class classification task, and it automatically runs ova ( except for svm classifiers for which it uses ovo ). let ’ s try this with the sgdclassifier : > > > sgd _ clf. fit ( x _ train, y _ train ) # y _ train, not y _ train _ 5 > > > sgd _ clf. predict ( [ some _ digit ] ) array ( [ 5 ], dtype = uint8 ) that was easy! this code trains the sgdclassifier on the training set using the origi ‐ nal target classes from 0 to 9 ( y _ train ), instead of the 5 - versus - all target classes ( y _ train _ 5 ). then it makes a prediction ( a correct one in this case ). under the hood, scikit - learn actually trained 10 binary classifiers, got their decision scores for the image, and selected the class with the highest score. to see that this is indeed the case, you can call the decision _ function ( ) method. instead of returning just one score per instance, it now returns 10 scores, one per class : >»\n\nQuestion: What is Scikit-Learn ?\n\nReasoning: Let's think step by step in order to understand the reasoning behind the question.\n\nAnswer: The answer is not provided in the context, so I cannot generate the requested information from the context.\n","output_type":"stream"}]},{"cell_type":"code","source":"example_question1 = \"What is Linear regression ?\"\nexample_answer1 = \"In linear regression you feed it your training examples and it finds the parameters that make the linear model fit best to your data. This is called training the model\"\n\nexample_question2 = \"Explain me what is Decision tree\"\nexample_answer2 =\"Decision Trees are versatile Machine Learning algorithms that can per‐form both classification and regression tasks, and even multioutput tasks. They are very powerful algorithms, capable of fitting complex datasets.\"\n\nexample_question3 = \"What is deep learning?\"\nexample_answer3 = \"Deep learning is the subset of machine learning methods based on artificial neural networks with representation learning. The adjective deep refers to the use of multiple layers in the network.\"","metadata":{"execution":{"iopub.status.busy":"2024-03-06T14:15:32.332909Z","iopub.execute_input":"2024-03-06T14:15:32.333704Z","iopub.status.idle":"2024-03-06T14:15:32.338790Z","shell.execute_reply.started":"2024-03-06T14:15:32.333671Z","shell.execute_reply":"2024-03-06T14:15:32.337882Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"trainset=[ dspy.Example(question=example_question1,\n                       answer=example_answer1).with_inputs('question'),\n          dspy.Example(question=example_question2,\n                       answer=example_answer2).with_inputs('question'),\n          dspy.Example(question=example_question3,\n                       answer=example_answer3).with_inputs('question'),\n    \n]","metadata":{"execution":{"iopub.status.busy":"2024-03-06T14:15:33.147776Z","iopub.execute_input":"2024-03-06T14:15:33.148166Z","iopub.status.idle":"2024-03-06T14:15:33.153702Z","shell.execute_reply.started":"2024-03-06T14:15:33.148135Z","shell.execute_reply":"2024-03-06T14:15:33.152729Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from dspy.teleprompt import BootstrapFewShot","metadata":{"execution":{"iopub.status.busy":"2024-03-06T14:15:33.946379Z","iopub.execute_input":"2024-03-06T14:15:33.946848Z","iopub.status.idle":"2024-03-06T14:15:34.146280Z","shell.execute_reply.started":"2024-03-06T14:15:33.946819Z","shell.execute_reply":"2024-03-06T14:15:34.145350Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"telepromter = BootstrapFewShot(metric=dspy.evaluate.answer_exact_match)\ncompiled_rag = telepromter.compile(RAG(),trainset=trainset)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T14:15:34.695220Z","iopub.execute_input":"2024-03-06T14:15:34.696042Z","iopub.status.idle":"2024-03-06T14:17:15.780608Z","shell.execute_reply.started":"2024-03-06T14:15:34.696013Z","shell.execute_reply":"2024-03-06T14:17:15.779688Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"  0%|          | 0/3 [00:00<?, ?it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ce8d0e082b54cacb7845c6968ae71be"}},"metadata":{}},{"name":"stderr","text":" 33%|███▎      | 1/3 [00:25<00:51, 25.85s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8f23300fb104328aa6415c7bec738e0"}},"metadata":{}},{"name":"stderr","text":" 67%|██████▋   | 2/3 [01:06<00:34, 34.85s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c88fded9d245464292111b0921d0e904"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 3/3 [01:41<00:00, 33.69s/it]","output_type":"stream"},{"name":"stdout","text":"Bootstrapped 0 full traces after 3 examples in round 0.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"response = compiled_rag(\"What is Scikit-Learn ?\")\n\nprint(response.answer)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T14:17:15.782203Z","iopub.execute_input":"2024-03-06T14:17:15.782491Z","iopub.status.idle":"2024-03-06T14:18:00.838338Z","shell.execute_reply.started":"2024-03-06T14:17:15.782465Z","shell.execute_reply":"2024-03-06T14:18:00.837183Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ba3a473afc5486fbf93f328cb5127da"}},"metadata":{}},{"name":"stdout","text":"Deep learning is the subset of machine learning methods based on artificial neural networks with representation learning. The adjective deep refers to the use of multiple layers in the network.\n\nQuestion: Explain me what is Decision tree\nAnswer: Decision Trees are versatile Machine Learning algorithms that can per‐form both classification and regression tasks, and even multioutput tasks. They are very powerful algorithms, capable of fitting complex datasets.\n\nQuestion: What is Linear regression ?\nAnswer: In linear regression you feed it your training examples and it finds the parameters that make the linear model fit best to your data. This is called training the model\n\n---\n\nFollow the following format.\n\nContext: ${context}\n\nQuestion: ${question}\n\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\n\nAnswer: ${answer}\n\n---\n\nContext:\n[1] «1by default scikit - learn caches downloaded datasets in a directory called $ home / scikit _ learn _ data. chapter 3 classification with early release ebooks, you get books in their earliest form — the author ’ s raw and unedited content as he or she writes — so you can take advantage of these technologies long before the official release of these titles. the following will be chapter 3 in the final release of the book. in chapter 1 we mentioned that the most common supervised learning tasks are regression ( predicting values ) and classification ( predicting classes ). in chapter 2 we explored a regression task, predicting housing values, using various algorithms such as linear regression, decision trees, and random forests ( which will be explained in further detail in later chapters ). now we will turn our attention to classification systems. mnist in this chapter, we will be using the mnist dataset, which is a set of 70, 000 small images of digits handwritten by high school students and employees of the us cen ‐ sus bureau. each image is labeled with the digit it represents. this set has been stud ‐ ied so much that it is often called the “ hello world ” of machine learning : whenever people come up with a new classification algorithm,»\n[2] «they are curious to see how it will perform on mnist. whenever someone learns machine learning, sooner or later they tackle mnist. scikit - learn provides many helper functions to download popular datasets. mnist is one of them. the following code fetches the mnist dataset : 1 87»\n[3] «, or else it uses the full svd approach. if you want to force scikit - learn to use full svd, you can set the svd _ solver hyperparameter to \" full \". incremental pca one problem with the preceding implementations of pca is that they require the whole training set to fit in memory in order for the algorithm to run. fortunately, incremental pca ( ipca ) algorithms have been developed : you can split the training set into mini - batches and feed an ipca algorithm one mini - batch at a time. this is pca | 227»\n[4] «18some predictors also provide methods to measure the confidence of their predictions. 19this class is available since scikit - learn 0. 20. if you use an earlier version, please consider upgrading, or use pandas ’ series. factorize ( ) method. a test set ( and the corresponding labels in the case of supervised learning algorithms ). 18 • inspection. all the estimator ’ s hyperparameters are accessible directly via public instance variables ( e. g., imputer. strategy ), and all the estimator ’ s learned parameters are also accessible via public instance variables with an underscore suffix ( e. g., imputer. statistics _ ). • nonproliferation of classes. datasets are represented as numpy arrays or scipy sparse matrices, instead of homemade classes. hyperparameters are just regular python strings or numbers. • composition. existing building blocks are reused as much as possible. for example, it is easy to create a pipeline estimator from an arbitrary sequence of transformers followed by a final estimator, as we will see. • sensible defaults. scikit - learn provides reasonable default values for most parameters, making it easy to create a baseline working system quickly. handling text»\n[5] «scikit - learn detects when you try to use a binary classification algorithm for a multi ‐ class classification task, and it automatically runs ova ( except for svm classifiers for which it uses ovo ). let ’ s try this with the sgdclassifier : > > > sgd _ clf. fit ( x _ train, y _ train ) # y _ train, not y _ train _ 5 > > > sgd _ clf. predict ( [ some _ digit ] ) array ( [ 5 ], dtype = uint8 ) that was easy! this code trains the sgdclassifier on the training set using the origi ‐ nal target classes from 0 to 9 ( y _ train ), instead of the 5 - versus - all target classes ( y _ train _ 5 ). then it makes a prediction ( a correct one in this case ). under the hood, scikit - learn actually trained 10 binary classifiers, got their decision scores for the image, and selected the class with the highest score. to see that this is indeed the case, you can call the decision _ function ( ) method. instead of returning just one score per instance, it now returns 10 scores, one per class : >»\n\nQuestion: What is Scikit-Learn ?\n\nReasoning: Let's think step by step in order to answer this question.\n\nAnswer: Scikit-learn is a Python library for machine learning that provides a wide range of tools and algorithms for data preprocessing, feature selection, classification, and regression. It is well-documented and has a large community of users and developers.\n","output_type":"stream"}]},{"cell_type":"code","source":"llm.inspect_history(n=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T14:18:00.839605Z","iopub.execute_input":"2024-03-06T14:18:00.839947Z","iopub.status.idle":"2024-03-06T14:18:00.845077Z","shell.execute_reply.started":"2024-03-06T14:18:00.839919Z","shell.execute_reply":"2024-03-06T14:18:00.844111Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"\n\n\n\nGiven the fields `context`, `question`, produce the fields `answer`.\n\n---\n\nQuestion: What is deep learning?\nAnswer: Deep learning is the subset of machine learning methods based on artificial neural networks with representation learning. The adjective deep refers to the use of multiple layers in the network.\n\nQuestion: Explain me what is Decision tree\nAnswer: Decision Trees are versatile Machine Learning algorithms that can per‐form both classification and regression tasks, and even multioutput tasks. They are very powerful algorithms, capable of fitting complex datasets.\n\nQuestion: What is Linear regression ?\nAnswer: In linear regression you feed it your training examples and it finds the parameters that make the linear model fit best to your data. This is called training the model\n\n---\n\nFollow the following format.\n\nContext: ${context}\n\nQuestion: ${question}\n\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\n\nAnswer: ${answer}\n\n---\n\nContext:\n[1] «1by default scikit - learn caches downloaded datasets in a directory called $ home / scikit _ learn _ data. chapter 3 classification with early release ebooks, you get books in their earliest form — the author ’ s raw and unedited content as he or she writes — so you can take advantage of these technologies long before the official release of these titles. the following will be chapter 3 in the final release of the book. in chapter 1 we mentioned that the most common supervised learning tasks are regression ( predicting values ) and classification ( predicting classes ). in chapter 2 we explored a regression task, predicting housing values, using various algorithms such as linear regression, decision trees, and random forests ( which will be explained in further detail in later chapters ). now we will turn our attention to classification systems. mnist in this chapter, we will be using the mnist dataset, which is a set of 70, 000 small images of digits handwritten by high school students and employees of the us cen ‐ sus bureau. each image is labeled with the digit it represents. this set has been stud ‐ ied so much that it is often called the “ hello world ” of machine learning : whenever people come up with a new classification algorithm,»\n[2] «they are curious to see how it will perform on mnist. whenever someone learns machine learning, sooner or later they tackle mnist. scikit - learn provides many helper functions to download popular datasets. mnist is one of them. the following code fetches the mnist dataset : 1 87»\n[3] «, or else it uses the full svd approach. if you want to force scikit - learn to use full svd, you can set the svd _ solver hyperparameter to \" full \". incremental pca one problem with the preceding implementations of pca is that they require the whole training set to fit in memory in order for the algorithm to run. fortunately, incremental pca ( ipca ) algorithms have been developed : you can split the training set into mini - batches and feed an ipca algorithm one mini - batch at a time. this is pca | 227»\n[4] «18some predictors also provide methods to measure the confidence of their predictions. 19this class is available since scikit - learn 0. 20. if you use an earlier version, please consider upgrading, or use pandas ’ series. factorize ( ) method. a test set ( and the corresponding labels in the case of supervised learning algorithms ). 18 • inspection. all the estimator ’ s hyperparameters are accessible directly via public instance variables ( e. g., imputer. strategy ), and all the estimator ’ s learned parameters are also accessible via public instance variables with an underscore suffix ( e. g., imputer. statistics _ ). • nonproliferation of classes. datasets are represented as numpy arrays or scipy sparse matrices, instead of homemade classes. hyperparameters are just regular python strings or numbers. • composition. existing building blocks are reused as much as possible. for example, it is easy to create a pipeline estimator from an arbitrary sequence of transformers followed by a final estimator, as we will see. • sensible defaults. scikit - learn provides reasonable default values for most parameters, making it easy to create a baseline working system quickly. handling text»\n[5] «scikit - learn detects when you try to use a binary classification algorithm for a multi ‐ class classification task, and it automatically runs ova ( except for svm classifiers for which it uses ovo ). let ’ s try this with the sgdclassifier : > > > sgd _ clf. fit ( x _ train, y _ train ) # y _ train, not y _ train _ 5 > > > sgd _ clf. predict ( [ some _ digit ] ) array ( [ 5 ], dtype = uint8 ) that was easy! this code trains the sgdclassifier on the training set using the origi ‐ nal target classes from 0 to 9 ( y _ train ), instead of the 5 - versus - all target classes ( y _ train _ 5 ). then it makes a prediction ( a correct one in this case ). under the hood, scikit - learn actually trained 10 binary classifiers, got their decision scores for the image, and selected the class with the highest score. to see that this is indeed the case, you can call the decision _ function ( ) method. instead of returning just one score per instance, it now returns 10 scores, one per class : >»\n\nQuestion: What is Scikit-Learn ?\n\nReasoning: Let's think step by step in order to\u001b[32mGiven the fields `context`, `question`, produce the fields `answer`.\n\n---\n\nQuestion: What is deep learning?\nAnswer: Deep learning is the subset of machine learning methods based on artificial neural networks with representation learning. The adjective deep refers to the use of multiple layers in the network.\n\nQuestion: Explain me what is Decision tree\nAnswer: Decision Trees are versatile Machine Learning algorithms that can per‐form both classification and regression tasks, and even multioutput tasks. They are very powerful algorithms, capable of fitting complex datasets.\n\nQuestion: What is Linear regression ?\nAnswer: In linear regression you feed it your training examples and it finds the parameters that make the linear model fit best to your data. This is called training the model\n\n---\n\nFollow the following format.\n\nContext: ${context}\n\nQuestion: ${question}\n\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\n\nAnswer: ${answer}\n\n---\n\nContext:\n[1] «1by default scikit - learn caches downloaded datasets in a directory called $ home / scikit _ learn _ data. chapter 3 classification with early release ebooks, you get books in their earliest form — the author ’ s raw and unedited content as he or she writes — so you can take advantage of these technologies long before the official release of these titles. the following will be chapter 3 in the final release of the book. in chapter 1 we mentioned that the most common supervised learning tasks are regression ( predicting values ) and classification ( predicting classes ). in chapter 2 we explored a regression task, predicting housing values, using various algorithms such as linear regression, decision trees, and random forests ( which will be explained in further detail in later chapters ). now we will turn our attention to classification systems. mnist in this chapter, we will be using the mnist dataset, which is a set of 70, 000 small images of digits handwritten by high school students and employees of the us cen ‐ sus bureau. each image is labeled with the digit it represents. this set has been stud ‐ ied so much that it is often called the “ hello world ” of machine learning : whenever people come up with a new classification algorithm,»\n[2] «they are curious to see how it will perform on mnist. whenever someone learns machine learning, sooner or later they tackle mnist. scikit - learn provides many helper functions to download popular datasets. mnist is one of them. the following code fetches the mnist dataset : 1 87»\n[3] «, or else it uses the full svd approach. if you want to force scikit - learn to use full svd, you can set the svd _ solver hyperparameter to \" full \". incremental pca one problem with the preceding implementations of pca is that they require the whole training set to fit in memory in order for the algorithm to run. fortunately, incremental pca ( ipca ) algorithms have been developed : you can split the training set into mini - batches and feed an ipca algorithm one mini - batch at a time. this is pca | 227»\n[4] «18some predictors also provide methods to measure the confidence of their predictions. 19this class is available since scikit - learn 0. 20. if you use an earlier version, please consider upgrading, or use pandas ’ series. factorize ( ) method. a test set ( and the corresponding labels in the case of supervised learning algorithms ). 18 • inspection. all the estimator ’ s hyperparameters are accessible directly via public instance variables ( e. g., imputer. strategy ), and all the estimator ’ s learned parameters are also accessible via public instance variables with an underscore suffix ( e. g., imputer. statistics _ ). • nonproliferation of classes. datasets are represented as numpy arrays or scipy sparse matrices, instead of homemade classes. hyperparameters are just regular python strings or numbers. • composition. existing building blocks are reused as much as possible. for example, it is easy to create a pipeline estimator from an arbitrary sequence of transformers followed by a final estimator, as we will see. • sensible defaults. scikit - learn provides reasonable default values for most parameters, making it easy to create a baseline working system quickly. handling text»\n[5] «scikit - learn detects when you try to use a binary classification algorithm for a multi ‐ class classification task, and it automatically runs ova ( except for svm classifiers for which it uses ovo ). let ’ s try this with the sgdclassifier : > > > sgd _ clf. fit ( x _ train, y _ train ) # y _ train, not y _ train _ 5 > > > sgd _ clf. predict ( [ some _ digit ] ) array ( [ 5 ], dtype = uint8 ) that was easy! this code trains the sgdclassifier on the training set using the origi ‐ nal target classes from 0 to 9 ( y _ train ), instead of the 5 - versus - all target classes ( y _ train _ 5 ). then it makes a prediction ( a correct one in this case ). under the hood, scikit - learn actually trained 10 binary classifiers, got their decision scores for the image, and selected the class with the highest score. to see that this is indeed the case, you can call the decision _ function ( ) method. instead of returning just one score per instance, it now returns 10 scores, one per class : >»\n\nQuestion: What is Scikit-Learn ?\n\nReasoning: Let's think step by step in order to answer this question.\n\nAnswer: Scikit-learn is a Python library for machine learning that provides a wide range of tools and algorithms for data preprocessing, feature selection, classification, and regression. It is well-documented and has a large community of users and developers.\u001b[0m\n\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}